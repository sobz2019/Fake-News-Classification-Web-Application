{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e81a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 00:24:23.149705: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-20 00:24:23.351494: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:24:23.351568: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /gpfs/home/psc21zcu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /gpfs/home/psc21zcu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /gpfs/home/psc21zcu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /gpfs/home/psc21zcu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import json\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tldextract   # Accurately separates a URL's subdomain, domain, and public suffix\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from collections import Counter\n",
    "import nltk \n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator,STOPWORDS\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_stopwords\n",
    "import copy\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,LancasterStemmer\n",
    "from sklearn.metrics import f1_score\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f8fa6",
   "metadata": {},
   "source": [
    "#### Loading the ISOT Full Dataset  -- After combined fake and real news  in the previous notebook   \"ISOT_ML_Classifiers_TfiDF_Tokeniser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa94991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_full_df = pd.read_csv(\"Updated//ISOT_Combined_FullData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d71bdb",
   "metadata": {},
   "source": [
    "#### Dropping the columns 'title' , 'text' and 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20450fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_full_df = isot_full_df.drop(columns = ['title','text', 'subject','date','title_length','body_length','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b503e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "isot_full_df=isot_full_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475fe800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>fulltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Macron vows Caribbean rebuild as anger rises against European powers French President Emmanuel Macron vowed to quickly rebuild the islands of the French Caribbean during a visit on Tuesday meant to dispel anger at his government s response to Hurricane Irma, which killed at least 43 people in the region.  The clutch of Caribbean islands hardest hit by the storm were mainly overseas territories belonging to Britain, France and the Netherlands, whose tens of thousands of residents are European Union citizens. The U.S. Virgin Islands were also hard hit. European countries and the United States have sent troops to deliver aid and provide security after the storm toppled homes and hospitals, but locals and tourists short of food or shelter say help was slow to arrive. Macron, who is also facing the first test at home of his resolve to reform the economy with a day of protests against his labor reforms, denied that authorities reacted too slowly. Basic services in the region were lost af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Cruz, Rubio say Iran prisoner swap a 'dangerous precedent' Republican presidential candidates Ted Cruz and Marco Rubio praised Iran’s release of five detained Americans on Sunday, but sharply criticized the deal the White House made to win their freedom saying it would lead to more Americans being taken “hostage.” U.S. President Barack Obama pardoned three Iranian-Americans charged for violating sanctions against Iran, a lawyer for one of the men said, while prosecutors moved to drop charges against four Iranians outside the United States. Iran agreed to free five Americans including Rezaian and Saeed Abedini, an Iranian-American Christian pastor sentenced to eight years in prison in 2013 on charges of undermining Iran’s national security.          Cruz, speaking to Fox News Sunday, said, “Praise God that the prisoners are coming home” but Iran got the better end of the prisoner swap. “We released seven terrorists who had helped Iran with their nuclear program, and we agreed not to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 fulltext\n",
       "0      0  Macron vows Caribbean rebuild as anger rises against European powers French President Emmanuel Macron vowed to quickly rebuild the islands of the French Caribbean during a visit on Tuesday meant to dispel anger at his government s response to Hurricane Irma, which killed at least 43 people in the region.  The clutch of Caribbean islands hardest hit by the storm were mainly overseas territories belonging to Britain, France and the Netherlands, whose tens of thousands of residents are European Union citizens. The U.S. Virgin Islands were also hard hit. European countries and the United States have sent troops to deliver aid and provide security after the storm toppled homes and hospitals, but locals and tourists short of food or shelter say help was slow to arrive. Macron, who is also facing the first test at home of his resolve to reform the economy with a day of protests against his labor reforms, denied that authorities reacted too slowly. Basic services in the region were lost af...\n",
       "1      0  Cruz, Rubio say Iran prisoner swap a 'dangerous precedent' Republican presidential candidates Ted Cruz and Marco Rubio praised Iran’s release of five detained Americans on Sunday, but sharply criticized the deal the White House made to win their freedom saying it would lead to more Americans being taken “hostage.” U.S. President Barack Obama pardoned three Iranian-Americans charged for violating sanctions against Iran, a lawyer for one of the men said, while prosecutors moved to drop charges against four Iranians outside the United States. Iran agreed to free five Americans including Rezaian and Saeed Abedini, an Iranian-American Christian pastor sentenced to eight years in prison in 2013 on charges of undermining Iran’s national security.          Cruz, speaking to Fox News Sunday, said, “Praise God that the prisoners are coming home” but Iran got the better end of the prisoner swap. “We released seven terrorists who had helped Iran with their nuclear program, and we agreed not to..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isot_full_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08647ab",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2deb574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "#https://androidkt.com/simple-text-classification-using-bert-in-tensorflow-keras-2-0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "635f9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=isot_full_df['fulltext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4e45e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 00:16:47.412807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:16:47.413205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:16:47.413505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:16:47.607965: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:16:47.608307: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:16:47.608612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /gpfs/software/ada/python/anaconda/2020.11/3.8/lib:/gpfs/software/ada/cuda/10.2.89/lib64:/gpfs/software/ada/cuda/10.2.89/lib/lib64\n",
      "2022-06-20 00:16:47.608685: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-20 00:16:47.610197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# bert preprocessor - https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
    "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "# bert encoder - https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/2\n",
    "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/2\",trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1267d",
   "metadata": {},
   "source": [
    "#### First set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef52e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing dataset  - First Set\n",
    "inputs = preprocessor(texts[0:10000])\n",
    "# feeding it to model for vectorization\n",
    "outputs = encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0581f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 512].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 512]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dac6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dataframe\n",
    "bertf_df1=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "977eb3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values added in dataframe\n"
     ]
    }
   ],
   "source": [
    "## Converting bert encoder sequence output to 1 dimension for ML Model training\n",
    "\n",
    "for i in range(0,len(sequence_output)):\n",
    "    b=sequence_output[i].numpy().sum(axis=0)\n",
    "    bertf_df1=bertf_df1.append(pd.Series(b),ignore_index=True)\n",
    "print('values added in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "550c03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertf_df1.to_csv('Updated//bertf_df1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b17f5b",
   "metadata": {},
   "source": [
    "#### Second Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ec6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing dataset  - First Set\n",
    "inputs2 = preprocessor(texts[10000:20000])\n",
    "# feeding it to model for vectorization\n",
    "outputs2 = encoder(inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f01e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output2 = outputs2[\"pooled_output\"]      # [batch_size, 512].\n",
    "sequence_output2 = outputs2[\"sequence_output\"]  # [batch_size, seq_length, 512]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01262aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dataframe\n",
    "bertf_df2=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "554da319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values added in dataframe\n"
     ]
    }
   ],
   "source": [
    "## Converting bert encoder sequence output to 1 dimension for ML Model training\n",
    "\n",
    "for i in range(0,len(sequence_output2)):\n",
    "    b=sequence_output2[i].numpy().sum(axis=0)\n",
    "    bertf_df2=bertf_df2.append(pd.Series(b),ignore_index=True)\n",
    "print('values added in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e650920",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertf_df2.to_csv('Updated//bertf_df2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2d064",
   "metadata": {},
   "source": [
    "#### Third Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79f0926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing dataset  - First Set\n",
    "inputs3 = preprocessor(texts[20000:30000])\n",
    "# feeding it to model for vectorization\n",
    "outputs3 = encoder(inputs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636ee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output3 = outputs3[\"pooled_output\"]      # [batch_size, 512].\n",
    "sequence_output3 = outputs3[\"sequence_output\"]  # [batch_size, seq_length, 512]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "064daa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dataframe\n",
    "bertf_df3=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91e20b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values added in dataframe\n"
     ]
    }
   ],
   "source": [
    "## Converting bert encoder sequence output to 1 dimension for ML Model training\n",
    "\n",
    "for i in range(0,len(sequence_output3)):\n",
    "    b=sequence_output3[i].numpy().sum(axis=0)\n",
    "    bertf_df3=bertf_df3.append(pd.Series(b),ignore_index=True)\n",
    "print('values added in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e1b4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertf_df3.to_csv('Updated//bertf_df3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe9ea5",
   "metadata": {},
   "source": [
    "#### Fourth Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f304752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing dataset  - First Set\n",
    "inputs4 = preprocessor(texts[30000:40000])\n",
    "# feeding it to model for vectorization\n",
    "outputs4 = encoder(inputs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace5f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output4 = outputs4[\"pooled_output\"]      # [batch_size, 512].\n",
    "sequence_output4 = outputs4[\"sequence_output\"]  # [batch_size, seq_length, 512]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2017f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining dataframe\n",
    "bertf_df4=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94cc0190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values added in dataframe\n"
     ]
    }
   ],
   "source": [
    "## Converting bert encoder sequence output to 1 dimension for ML Model training\n",
    "\n",
    "for i in range(0,len(sequence_output4)):\n",
    "    b=sequence_output4[i].numpy().sum(axis=0)\n",
    "    bertf_df4=bertf_df4.append(pd.Series(b),ignore_index=True)\n",
    "print('values added in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e6b5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertf_df4.to_csv('Updated//bertf_df4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de7599",
   "metadata": {},
   "source": [
    "### Combining all the above 4 sets into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de65e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertf_df1 = pd.read_csv(\"Updated//bertf_df1.csv\")\n",
    "bertf_df2 = pd.read_csv(\"Updated//bertf_df2.csv\")\n",
    "bertf_df3 = pd.read_csv(\"Updated//bertf_df3.csv\")\n",
    "bertf_df4 = pd.read_csv(\"Updated//bertf_df4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7846da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both props \n",
    "bertVectors_fulldf=pd.concat([bertf_df1,bertf_df2,bertf_df3,bertf_df4])\n",
    "bertVectors_fulldf.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1c069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa00d528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38270, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertVectors_fulldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4a408b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20820\n",
       "1    17450\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isot_full_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cede48",
   "metadata": {},
   "source": [
    "### Adding class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0413828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertVectors_fulldf.insert(len(bertVectors_fulldf.columns),'class',isot_full_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fff5d4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>...</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.076153</td>\n",
       "      <td>-29.068226</td>\n",
       "      <td>57.540318</td>\n",
       "      <td>-51.954987</td>\n",
       "      <td>3.320737</td>\n",
       "      <td>-5.768854</td>\n",
       "      <td>79.962100</td>\n",
       "      <td>39.567856</td>\n",
       "      <td>-17.904541</td>\n",
       "      <td>-13.444562</td>\n",
       "      <td>7.104392</td>\n",
       "      <td>-55.4440</td>\n",
       "      <td>-24.303596</td>\n",
       "      <td>-44.187996</td>\n",
       "      <td>2.927974</td>\n",
       "      <td>53.233093</td>\n",
       "      <td>50.92992</td>\n",
       "      <td>-43.447510</td>\n",
       "      <td>-89.645250</td>\n",
       "      <td>22.505268</td>\n",
       "      <td>-60.829258</td>\n",
       "      <td>-20.836334</td>\n",
       "      <td>-0.965031</td>\n",
       "      <td>31.152092</td>\n",
       "      <td>-11.122220</td>\n",
       "      <td>-25.034784</td>\n",
       "      <td>4.409427</td>\n",
       "      <td>26.100441</td>\n",
       "      <td>-25.010174</td>\n",
       "      <td>-74.720795</td>\n",
       "      <td>53.209137</td>\n",
       "      <td>-81.84636</td>\n",
       "      <td>50.065517</td>\n",
       "      <td>-45.970085</td>\n",
       "      <td>30.723158</td>\n",
       "      <td>27.164768</td>\n",
       "      <td>-28.826418</td>\n",
       "      <td>-6.222152</td>\n",
       "      <td>6.740042</td>\n",
       "      <td>40.17959</td>\n",
       "      <td>44.086067</td>\n",
       "      <td>19.706493</td>\n",
       "      <td>20.024275</td>\n",
       "      <td>-64.465230</td>\n",
       "      <td>59.972400</td>\n",
       "      <td>-8.002469</td>\n",
       "      <td>13.231766</td>\n",
       "      <td>92.479520</td>\n",
       "      <td>59.146214</td>\n",
       "      <td>36.834324</td>\n",
       "      <td>-39.294914</td>\n",
       "      <td>38.488094</td>\n",
       "      <td>-15.634628</td>\n",
       "      <td>5.950771</td>\n",
       "      <td>64.343360</td>\n",
       "      <td>-57.710840</td>\n",
       "      <td>-31.729551</td>\n",
       "      <td>17.203966</td>\n",
       "      <td>6.631249</td>\n",
       "      <td>42.381054</td>\n",
       "      <td>2.142177</td>\n",
       "      <td>8.529372</td>\n",
       "      <td>-37.259970</td>\n",
       "      <td>6.108812</td>\n",
       "      <td>-27.347390</td>\n",
       "      <td>26.856197</td>\n",
       "      <td>-21.978996</td>\n",
       "      <td>-52.820110</td>\n",
       "      <td>53.621487</td>\n",
       "      <td>2.217431</td>\n",
       "      <td>81.62559</td>\n",
       "      <td>19.125967</td>\n",
       "      <td>-12.114867</td>\n",
       "      <td>114.19409</td>\n",
       "      <td>22.398640</td>\n",
       "      <td>-26.302088</td>\n",
       "      <td>9.557514</td>\n",
       "      <td>-63.690918</td>\n",
       "      <td>-6.997769</td>\n",
       "      <td>-72.960580</td>\n",
       "      <td>-62.32190</td>\n",
       "      <td>38.198235</td>\n",
       "      <td>-22.560627</td>\n",
       "      <td>58.402110</td>\n",
       "      <td>6.116537</td>\n",
       "      <td>-55.134212</td>\n",
       "      <td>10.474427</td>\n",
       "      <td>-24.749079</td>\n",
       "      <td>24.216830</td>\n",
       "      <td>76.149510</td>\n",
       "      <td>-57.48442</td>\n",
       "      <td>44.822865</td>\n",
       "      <td>10.780519</td>\n",
       "      <td>-26.609621</td>\n",
       "      <td>-5.474360</td>\n",
       "      <td>-58.219970</td>\n",
       "      <td>-2.624526</td>\n",
       "      <td>-48.936640</td>\n",
       "      <td>-8.839552</td>\n",
       "      <td>115.739075</td>\n",
       "      <td>11.119407</td>\n",
       "      <td>43.98946</td>\n",
       "      <td>64.425964</td>\n",
       "      <td>-41.28047</td>\n",
       "      <td>26.402365</td>\n",
       "      <td>47.93493</td>\n",
       "      <td>-28.618982</td>\n",
       "      <td>-32.531040</td>\n",
       "      <td>25.251757</td>\n",
       "      <td>7.147524</td>\n",
       "      <td>-20.038187</td>\n",
       "      <td>-6.329055</td>\n",
       "      <td>-82.623290</td>\n",
       "      <td>-19.123924</td>\n",
       "      <td>19.217548</td>\n",
       "      <td>24.678738</td>\n",
       "      <td>-46.619442</td>\n",
       "      <td>-16.176466</td>\n",
       "      <td>53.027214</td>\n",
       "      <td>-56.030000</td>\n",
       "      <td>-5.062048</td>\n",
       "      <td>-20.987423</td>\n",
       "      <td>2.937074</td>\n",
       "      <td>-38.340145</td>\n",
       "      <td>10.470112</td>\n",
       "      <td>59.40322</td>\n",
       "      <td>-51.526802</td>\n",
       "      <td>22.722168</td>\n",
       "      <td>9.580302</td>\n",
       "      <td>71.415770</td>\n",
       "      <td>-54.483715</td>\n",
       "      <td>-29.626957</td>\n",
       "      <td>-5.157812</td>\n",
       "      <td>-62.49628</td>\n",
       "      <td>38.692795</td>\n",
       "      <td>-0.546269</td>\n",
       "      <td>-5.400824</td>\n",
       "      <td>-69.03352</td>\n",
       "      <td>34.485413</td>\n",
       "      <td>34.81170</td>\n",
       "      <td>4.540720</td>\n",
       "      <td>14.029618</td>\n",
       "      <td>1.117358</td>\n",
       "      <td>67.148766</td>\n",
       "      <td>54.817284</td>\n",
       "      <td>-21.200008</td>\n",
       "      <td>-47.789494</td>\n",
       "      <td>38.494070</td>\n",
       "      <td>-79.732940</td>\n",
       "      <td>37.797665</td>\n",
       "      <td>-44.171960</td>\n",
       "      <td>42.925446</td>\n",
       "      <td>31.51484</td>\n",
       "      <td>-22.553644</td>\n",
       "      <td>33.205990</td>\n",
       "      <td>-22.327332</td>\n",
       "      <td>24.549444</td>\n",
       "      <td>13.433725</td>\n",
       "      <td>-25.152117</td>\n",
       "      <td>22.379246</td>\n",
       "      <td>8.82339</td>\n",
       "      <td>-5.749085</td>\n",
       "      <td>-32.831110</td>\n",
       "      <td>14.075165</td>\n",
       "      <td>-13.587171</td>\n",
       "      <td>-24.091106</td>\n",
       "      <td>3.647905</td>\n",
       "      <td>-49.28312</td>\n",
       "      <td>-73.42274</td>\n",
       "      <td>34.545450</td>\n",
       "      <td>121.210620</td>\n",
       "      <td>6.579218</td>\n",
       "      <td>-14.331875</td>\n",
       "      <td>25.185430</td>\n",
       "      <td>-0.427416</td>\n",
       "      <td>25.58867</td>\n",
       "      <td>32.091415</td>\n",
       "      <td>60.210037</td>\n",
       "      <td>114.56279</td>\n",
       "      <td>-18.183249</td>\n",
       "      <td>109.43097</td>\n",
       "      <td>4.606071</td>\n",
       "      <td>-816.7640</td>\n",
       "      <td>34.536625</td>\n",
       "      <td>-1.968195</td>\n",
       "      <td>-28.142189</td>\n",
       "      <td>-35.985890</td>\n",
       "      <td>-26.224033</td>\n",
       "      <td>-14.747208</td>\n",
       "      <td>-44.331577</td>\n",
       "      <td>40.095245</td>\n",
       "      <td>72.64052</td>\n",
       "      <td>22.202890</td>\n",
       "      <td>44.85098</td>\n",
       "      <td>45.743100</td>\n",
       "      <td>-17.891928</td>\n",
       "      <td>-69.220380</td>\n",
       "      <td>9.521957</td>\n",
       "      <td>-46.55743</td>\n",
       "      <td>38.384500</td>\n",
       "      <td>-22.983368</td>\n",
       "      <td>36.421482</td>\n",
       "      <td>-1.850905</td>\n",
       "      <td>53.13959</td>\n",
       "      <td>27.446419</td>\n",
       "      <td>54.202442</td>\n",
       "      <td>-41.36680</td>\n",
       "      <td>-74.47336</td>\n",
       "      <td>-37.391410</td>\n",
       "      <td>26.398663</td>\n",
       "      <td>5.670536</td>\n",
       "      <td>-7.041967</td>\n",
       "      <td>-72.567800</td>\n",
       "      <td>59.946780</td>\n",
       "      <td>-25.505404</td>\n",
       "      <td>-37.007404</td>\n",
       "      <td>-20.950542</td>\n",
       "      <td>-89.56310</td>\n",
       "      <td>46.319607</td>\n",
       "      <td>39.164486</td>\n",
       "      <td>-8.790081</td>\n",
       "      <td>-44.477100</td>\n",
       "      <td>-7.110732</td>\n",
       "      <td>5.267138</td>\n",
       "      <td>38.50156</td>\n",
       "      <td>20.837385</td>\n",
       "      <td>-16.691378</td>\n",
       "      <td>15.983549</td>\n",
       "      <td>-34.441160</td>\n",
       "      <td>5.828382</td>\n",
       "      <td>-0.527457</td>\n",
       "      <td>-89.16502</td>\n",
       "      <td>-11.465632</td>\n",
       "      <td>-41.326492</td>\n",
       "      <td>48.355556</td>\n",
       "      <td>10.083818</td>\n",
       "      <td>-6.473544</td>\n",
       "      <td>56.162754</td>\n",
       "      <td>-7.291161</td>\n",
       "      <td>-43.13583</td>\n",
       "      <td>-7.445608</td>\n",
       "      <td>15.005452</td>\n",
       "      <td>-21.107140</td>\n",
       "      <td>-22.295748</td>\n",
       "      <td>54.071922</td>\n",
       "      <td>0.270911</td>\n",
       "      <td>-13.642708</td>\n",
       "      <td>-8.504085</td>\n",
       "      <td>73.680405</td>\n",
       "      <td>11.293367</td>\n",
       "      <td>...</td>\n",
       "      <td>44.443905</td>\n",
       "      <td>-6.000172</td>\n",
       "      <td>-13.519516</td>\n",
       "      <td>0.777792</td>\n",
       "      <td>-45.398483</td>\n",
       "      <td>21.852337</td>\n",
       "      <td>46.511040</td>\n",
       "      <td>47.308490</td>\n",
       "      <td>7.052961</td>\n",
       "      <td>-59.692270</td>\n",
       "      <td>-45.581566</td>\n",
       "      <td>-5.164234</td>\n",
       "      <td>-53.656662</td>\n",
       "      <td>-67.76812</td>\n",
       "      <td>56.04384</td>\n",
       "      <td>-8.995872</td>\n",
       "      <td>27.532810</td>\n",
       "      <td>-5.376409</td>\n",
       "      <td>4.35754</td>\n",
       "      <td>-17.950928</td>\n",
       "      <td>-34.384330</td>\n",
       "      <td>-33.412437</td>\n",
       "      <td>2.268966</td>\n",
       "      <td>-19.778896</td>\n",
       "      <td>32.21143</td>\n",
       "      <td>-96.01068</td>\n",
       "      <td>15.489947</td>\n",
       "      <td>-20.747654</td>\n",
       "      <td>-56.013233</td>\n",
       "      <td>-45.682384</td>\n",
       "      <td>2.426109</td>\n",
       "      <td>28.742384</td>\n",
       "      <td>-43.443092</td>\n",
       "      <td>-9.90637</td>\n",
       "      <td>41.973927</td>\n",
       "      <td>1.298070</td>\n",
       "      <td>60.387585</td>\n",
       "      <td>5.639635</td>\n",
       "      <td>33.382930</td>\n",
       "      <td>54.166416</td>\n",
       "      <td>-27.001698</td>\n",
       "      <td>-33.079388</td>\n",
       "      <td>32.099163</td>\n",
       "      <td>-13.426831</td>\n",
       "      <td>-25.494100</td>\n",
       "      <td>2.569302</td>\n",
       "      <td>-19.967075</td>\n",
       "      <td>27.034811</td>\n",
       "      <td>-27.587006</td>\n",
       "      <td>3.497190</td>\n",
       "      <td>-2.361070</td>\n",
       "      <td>18.953096</td>\n",
       "      <td>-35.018530</td>\n",
       "      <td>15.905002</td>\n",
       "      <td>52.972460</td>\n",
       "      <td>10.520672</td>\n",
       "      <td>-6.272368</td>\n",
       "      <td>21.481680</td>\n",
       "      <td>-49.247795</td>\n",
       "      <td>9.748928</td>\n",
       "      <td>-20.084112</td>\n",
       "      <td>-8.60420</td>\n",
       "      <td>44.910910</td>\n",
       "      <td>-18.825981</td>\n",
       "      <td>4.937567</td>\n",
       "      <td>-77.733635</td>\n",
       "      <td>-8.999736</td>\n",
       "      <td>22.257694</td>\n",
       "      <td>30.929598</td>\n",
       "      <td>-62.188572</td>\n",
       "      <td>-19.494125</td>\n",
       "      <td>24.403084</td>\n",
       "      <td>19.481556</td>\n",
       "      <td>8.760558</td>\n",
       "      <td>6.620137</td>\n",
       "      <td>-8.348998</td>\n",
       "      <td>50.268090</td>\n",
       "      <td>50.917107</td>\n",
       "      <td>17.995626</td>\n",
       "      <td>-48.919400</td>\n",
       "      <td>-14.037656</td>\n",
       "      <td>13.549075</td>\n",
       "      <td>-0.513817</td>\n",
       "      <td>-14.460697</td>\n",
       "      <td>35.509663</td>\n",
       "      <td>-21.761173</td>\n",
       "      <td>54.061058</td>\n",
       "      <td>27.399305</td>\n",
       "      <td>-18.618568</td>\n",
       "      <td>-3.328788</td>\n",
       "      <td>-104.211910</td>\n",
       "      <td>-0.355629</td>\n",
       "      <td>-64.095924</td>\n",
       "      <td>30.758144</td>\n",
       "      <td>0.901769</td>\n",
       "      <td>0.102869</td>\n",
       "      <td>-1.056369</td>\n",
       "      <td>-42.882874</td>\n",
       "      <td>-55.050808</td>\n",
       "      <td>15.005190</td>\n",
       "      <td>17.073736</td>\n",
       "      <td>-12.501462</td>\n",
       "      <td>-19.862846</td>\n",
       "      <td>-24.29204</td>\n",
       "      <td>35.211872</td>\n",
       "      <td>-11.243217</td>\n",
       "      <td>-12.755215</td>\n",
       "      <td>66.646645</td>\n",
       "      <td>-32.807500</td>\n",
       "      <td>-0.189058</td>\n",
       "      <td>-1.808646</td>\n",
       "      <td>27.252577</td>\n",
       "      <td>-35.074340</td>\n",
       "      <td>28.530462</td>\n",
       "      <td>-29.252598</td>\n",
       "      <td>-21.704420</td>\n",
       "      <td>11.736714</td>\n",
       "      <td>54.281868</td>\n",
       "      <td>-18.355640</td>\n",
       "      <td>-43.98375</td>\n",
       "      <td>5.979135</td>\n",
       "      <td>59.810116</td>\n",
       "      <td>24.799034</td>\n",
       "      <td>12.219104</td>\n",
       "      <td>56.225810</td>\n",
       "      <td>17.381193</td>\n",
       "      <td>-8.651945</td>\n",
       "      <td>8.412357</td>\n",
       "      <td>-40.141530</td>\n",
       "      <td>20.355501</td>\n",
       "      <td>22.359621</td>\n",
       "      <td>33.184193</td>\n",
       "      <td>-9.555579</td>\n",
       "      <td>-28.605461</td>\n",
       "      <td>-44.75495</td>\n",
       "      <td>-2.173939</td>\n",
       "      <td>-31.205430</td>\n",
       "      <td>4.260418</td>\n",
       "      <td>-1.839805</td>\n",
       "      <td>1.392822</td>\n",
       "      <td>7.33269</td>\n",
       "      <td>53.761410</td>\n",
       "      <td>-16.712246</td>\n",
       "      <td>-23.574620</td>\n",
       "      <td>-22.573244</td>\n",
       "      <td>24.96579</td>\n",
       "      <td>14.559516</td>\n",
       "      <td>-14.097199</td>\n",
       "      <td>-24.911524</td>\n",
       "      <td>-1.114475</td>\n",
       "      <td>-48.574276</td>\n",
       "      <td>11.136024</td>\n",
       "      <td>1.976270</td>\n",
       "      <td>-9.241048</td>\n",
       "      <td>-6.975331</td>\n",
       "      <td>103.856870</td>\n",
       "      <td>15.996011</td>\n",
       "      <td>46.73991</td>\n",
       "      <td>40.200363</td>\n",
       "      <td>24.503580</td>\n",
       "      <td>-127.010635</td>\n",
       "      <td>68.728720</td>\n",
       "      <td>-8.694561</td>\n",
       "      <td>-2.824552</td>\n",
       "      <td>-34.471985</td>\n",
       "      <td>21.793420</td>\n",
       "      <td>35.863308</td>\n",
       "      <td>105.52343</td>\n",
       "      <td>49.373283</td>\n",
       "      <td>-14.536914</td>\n",
       "      <td>12.671607</td>\n",
       "      <td>29.56023</td>\n",
       "      <td>41.027367</td>\n",
       "      <td>-22.932981</td>\n",
       "      <td>47.56592</td>\n",
       "      <td>-6.551812</td>\n",
       "      <td>76.19963</td>\n",
       "      <td>-7.192747</td>\n",
       "      <td>-14.825335</td>\n",
       "      <td>11.637272</td>\n",
       "      <td>11.242970</td>\n",
       "      <td>48.271664</td>\n",
       "      <td>16.139172</td>\n",
       "      <td>-45.867954</td>\n",
       "      <td>-52.42260</td>\n",
       "      <td>1.913857</td>\n",
       "      <td>-84.181510</td>\n",
       "      <td>-91.65905</td>\n",
       "      <td>102.68495</td>\n",
       "      <td>-63.827293</td>\n",
       "      <td>-0.242578</td>\n",
       "      <td>38.447304</td>\n",
       "      <td>24.781075</td>\n",
       "      <td>-59.375990</td>\n",
       "      <td>-7.122683</td>\n",
       "      <td>10.546757</td>\n",
       "      <td>7.254651</td>\n",
       "      <td>7.789337</td>\n",
       "      <td>-23.236930</td>\n",
       "      <td>28.699804</td>\n",
       "      <td>-19.570255</td>\n",
       "      <td>64.782646</td>\n",
       "      <td>56.858818</td>\n",
       "      <td>3.428119</td>\n",
       "      <td>42.64988</td>\n",
       "      <td>-25.013424</td>\n",
       "      <td>-33.71716</td>\n",
       "      <td>-14.538428</td>\n",
       "      <td>21.464668</td>\n",
       "      <td>74.353615</td>\n",
       "      <td>-42.33282</td>\n",
       "      <td>-9.540282</td>\n",
       "      <td>-20.122068</td>\n",
       "      <td>42.71078</td>\n",
       "      <td>-0.752739</td>\n",
       "      <td>-13.769701</td>\n",
       "      <td>1.050589</td>\n",
       "      <td>34.419098</td>\n",
       "      <td>29.317112</td>\n",
       "      <td>-38.019440</td>\n",
       "      <td>16.762892</td>\n",
       "      <td>9.687283</td>\n",
       "      <td>-55.027912</td>\n",
       "      <td>-35.494670</td>\n",
       "      <td>58.193813</td>\n",
       "      <td>-1.021184</td>\n",
       "      <td>-32.058918</td>\n",
       "      <td>44.992233</td>\n",
       "      <td>-0.932869</td>\n",
       "      <td>30.857075</td>\n",
       "      <td>-11.946971</td>\n",
       "      <td>6.736805</td>\n",
       "      <td>-10.248713</td>\n",
       "      <td>-4.045509</td>\n",
       "      <td>-20.385088</td>\n",
       "      <td>-15.440907</td>\n",
       "      <td>29.357740</td>\n",
       "      <td>-52.567047</td>\n",
       "      <td>35.755077</td>\n",
       "      <td>-52.902596</td>\n",
       "      <td>-19.132236</td>\n",
       "      <td>24.880768</td>\n",
       "      <td>31.330477</td>\n",
       "      <td>-17.345932</td>\n",
       "      <td>18.819689</td>\n",
       "      <td>-8.718418</td>\n",
       "      <td>4.374234</td>\n",
       "      <td>-51.202106</td>\n",
       "      <td>5.171546</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.869040</td>\n",
       "      <td>9.177978</td>\n",
       "      <td>9.704573</td>\n",
       "      <td>-71.219390</td>\n",
       "      <td>-5.944699</td>\n",
       "      <td>-20.537598</td>\n",
       "      <td>48.504128</td>\n",
       "      <td>59.785946</td>\n",
       "      <td>-27.258816</td>\n",
       "      <td>-32.251190</td>\n",
       "      <td>14.800838</td>\n",
       "      <td>-53.5956</td>\n",
       "      <td>-42.787678</td>\n",
       "      <td>-24.545757</td>\n",
       "      <td>-0.176815</td>\n",
       "      <td>19.415184</td>\n",
       "      <td>109.40230</td>\n",
       "      <td>22.388596</td>\n",
       "      <td>-46.119553</td>\n",
       "      <td>12.120032</td>\n",
       "      <td>-47.389282</td>\n",
       "      <td>-47.334510</td>\n",
       "      <td>-47.276146</td>\n",
       "      <td>20.823063</td>\n",
       "      <td>-23.905811</td>\n",
       "      <td>14.022844</td>\n",
       "      <td>-23.051254</td>\n",
       "      <td>53.248184</td>\n",
       "      <td>-53.657482</td>\n",
       "      <td>-87.474130</td>\n",
       "      <td>45.754990</td>\n",
       "      <td>-88.19240</td>\n",
       "      <td>41.973026</td>\n",
       "      <td>-32.760735</td>\n",
       "      <td>-23.768059</td>\n",
       "      <td>48.279350</td>\n",
       "      <td>-18.068876</td>\n",
       "      <td>-62.380000</td>\n",
       "      <td>-25.564194</td>\n",
       "      <td>61.33734</td>\n",
       "      <td>40.179916</td>\n",
       "      <td>20.387558</td>\n",
       "      <td>30.500923</td>\n",
       "      <td>-60.822758</td>\n",
       "      <td>102.107605</td>\n",
       "      <td>14.142278</td>\n",
       "      <td>-3.871582</td>\n",
       "      <td>42.111538</td>\n",
       "      <td>6.905591</td>\n",
       "      <td>4.958260</td>\n",
       "      <td>16.016796</td>\n",
       "      <td>7.780636</td>\n",
       "      <td>-0.848523</td>\n",
       "      <td>1.473870</td>\n",
       "      <td>57.682693</td>\n",
       "      <td>-49.853676</td>\n",
       "      <td>-19.702677</td>\n",
       "      <td>31.856785</td>\n",
       "      <td>-6.906637</td>\n",
       "      <td>13.091820</td>\n",
       "      <td>9.293388</td>\n",
       "      <td>-46.937810</td>\n",
       "      <td>-15.624641</td>\n",
       "      <td>34.178690</td>\n",
       "      <td>-8.953811</td>\n",
       "      <td>8.842891</td>\n",
       "      <td>-27.109920</td>\n",
       "      <td>-33.114067</td>\n",
       "      <td>53.697777</td>\n",
       "      <td>23.613077</td>\n",
       "      <td>54.73936</td>\n",
       "      <td>-19.626383</td>\n",
       "      <td>-10.744253</td>\n",
       "      <td>132.13037</td>\n",
       "      <td>47.727695</td>\n",
       "      <td>-5.566465</td>\n",
       "      <td>19.573042</td>\n",
       "      <td>-29.790215</td>\n",
       "      <td>14.902081</td>\n",
       "      <td>-118.077576</td>\n",
       "      <td>-66.97544</td>\n",
       "      <td>-5.061472</td>\n",
       "      <td>-26.892975</td>\n",
       "      <td>20.348545</td>\n",
       "      <td>-8.329653</td>\n",
       "      <td>-39.616898</td>\n",
       "      <td>46.656700</td>\n",
       "      <td>19.301174</td>\n",
       "      <td>25.580462</td>\n",
       "      <td>40.807236</td>\n",
       "      <td>-46.38029</td>\n",
       "      <td>58.307915</td>\n",
       "      <td>22.420975</td>\n",
       "      <td>-42.110580</td>\n",
       "      <td>-43.163948</td>\n",
       "      <td>-36.296097</td>\n",
       "      <td>-14.917341</td>\n",
       "      <td>-7.593774</td>\n",
       "      <td>18.175910</td>\n",
       "      <td>102.734770</td>\n",
       "      <td>17.544197</td>\n",
       "      <td>49.27179</td>\n",
       "      <td>108.209670</td>\n",
       "      <td>-51.23678</td>\n",
       "      <td>-16.084090</td>\n",
       "      <td>71.24989</td>\n",
       "      <td>-16.147522</td>\n",
       "      <td>-25.371195</td>\n",
       "      <td>-0.574685</td>\n",
       "      <td>38.962357</td>\n",
       "      <td>23.742937</td>\n",
       "      <td>-7.922504</td>\n",
       "      <td>-49.855923</td>\n",
       "      <td>-5.934124</td>\n",
       "      <td>0.081892</td>\n",
       "      <td>-3.771686</td>\n",
       "      <td>-74.046230</td>\n",
       "      <td>-50.868767</td>\n",
       "      <td>46.930702</td>\n",
       "      <td>23.048883</td>\n",
       "      <td>4.499016</td>\n",
       "      <td>-34.497400</td>\n",
       "      <td>-6.409646</td>\n",
       "      <td>19.599749</td>\n",
       "      <td>-2.481324</td>\n",
       "      <td>43.26506</td>\n",
       "      <td>-43.030330</td>\n",
       "      <td>-45.001186</td>\n",
       "      <td>35.294243</td>\n",
       "      <td>5.455137</td>\n",
       "      <td>-72.468380</td>\n",
       "      <td>-42.423283</td>\n",
       "      <td>-9.063019</td>\n",
       "      <td>-10.23430</td>\n",
       "      <td>37.078854</td>\n",
       "      <td>21.051090</td>\n",
       "      <td>-7.432247</td>\n",
       "      <td>-83.89124</td>\n",
       "      <td>25.783722</td>\n",
       "      <td>-30.41206</td>\n",
       "      <td>35.521923</td>\n",
       "      <td>17.499136</td>\n",
       "      <td>62.885590</td>\n",
       "      <td>23.285479</td>\n",
       "      <td>45.144318</td>\n",
       "      <td>-17.518013</td>\n",
       "      <td>-22.365147</td>\n",
       "      <td>44.583626</td>\n",
       "      <td>-30.976406</td>\n",
       "      <td>-25.191942</td>\n",
       "      <td>-30.897219</td>\n",
       "      <td>49.849342</td>\n",
       "      <td>48.11522</td>\n",
       "      <td>-5.820929</td>\n",
       "      <td>12.154154</td>\n",
       "      <td>-14.323309</td>\n",
       "      <td>50.116608</td>\n",
       "      <td>17.387451</td>\n",
       "      <td>-28.413244</td>\n",
       "      <td>2.304046</td>\n",
       "      <td>38.76875</td>\n",
       "      <td>15.832017</td>\n",
       "      <td>-11.301152</td>\n",
       "      <td>14.961336</td>\n",
       "      <td>-5.422130</td>\n",
       "      <td>-51.911636</td>\n",
       "      <td>55.641655</td>\n",
       "      <td>-36.55812</td>\n",
       "      <td>-78.60230</td>\n",
       "      <td>27.394987</td>\n",
       "      <td>121.336235</td>\n",
       "      <td>-37.578762</td>\n",
       "      <td>-14.832090</td>\n",
       "      <td>23.933712</td>\n",
       "      <td>18.173290</td>\n",
       "      <td>32.73622</td>\n",
       "      <td>44.363930</td>\n",
       "      <td>23.287400</td>\n",
       "      <td>84.99884</td>\n",
       "      <td>-13.705014</td>\n",
       "      <td>101.15838</td>\n",
       "      <td>18.020290</td>\n",
       "      <td>-841.8158</td>\n",
       "      <td>40.813717</td>\n",
       "      <td>16.592876</td>\n",
       "      <td>-18.084003</td>\n",
       "      <td>-32.998116</td>\n",
       "      <td>-41.458576</td>\n",
       "      <td>1.810637</td>\n",
       "      <td>-100.587960</td>\n",
       "      <td>-7.341225</td>\n",
       "      <td>79.67986</td>\n",
       "      <td>1.496814</td>\n",
       "      <td>20.61357</td>\n",
       "      <td>44.502132</td>\n",
       "      <td>0.104825</td>\n",
       "      <td>4.125615</td>\n",
       "      <td>-1.947869</td>\n",
       "      <td>-44.67108</td>\n",
       "      <td>29.651737</td>\n",
       "      <td>-30.925812</td>\n",
       "      <td>59.398655</td>\n",
       "      <td>35.231987</td>\n",
       "      <td>58.75975</td>\n",
       "      <td>46.080510</td>\n",
       "      <td>3.883729</td>\n",
       "      <td>-66.10764</td>\n",
       "      <td>-43.79035</td>\n",
       "      <td>-44.196465</td>\n",
       "      <td>19.362597</td>\n",
       "      <td>11.944158</td>\n",
       "      <td>-22.527617</td>\n",
       "      <td>-36.538193</td>\n",
       "      <td>25.043571</td>\n",
       "      <td>-30.555069</td>\n",
       "      <td>-79.037575</td>\n",
       "      <td>-36.023853</td>\n",
       "      <td>-58.09647</td>\n",
       "      <td>-6.596299</td>\n",
       "      <td>90.721985</td>\n",
       "      <td>-4.542720</td>\n",
       "      <td>-25.315422</td>\n",
       "      <td>28.636532</td>\n",
       "      <td>-23.953516</td>\n",
       "      <td>43.03536</td>\n",
       "      <td>9.448089</td>\n",
       "      <td>17.827640</td>\n",
       "      <td>21.234476</td>\n",
       "      <td>-6.425253</td>\n",
       "      <td>4.652818</td>\n",
       "      <td>14.221005</td>\n",
       "      <td>-73.50012</td>\n",
       "      <td>3.506240</td>\n",
       "      <td>-34.049060</td>\n",
       "      <td>68.434860</td>\n",
       "      <td>-15.345180</td>\n",
       "      <td>-47.916664</td>\n",
       "      <td>58.037685</td>\n",
       "      <td>12.731979</td>\n",
       "      <td>-38.65578</td>\n",
       "      <td>19.502857</td>\n",
       "      <td>16.036879</td>\n",
       "      <td>29.603859</td>\n",
       "      <td>-19.430283</td>\n",
       "      <td>-5.402806</td>\n",
       "      <td>-1.403189</td>\n",
       "      <td>-32.265370</td>\n",
       "      <td>7.675438</td>\n",
       "      <td>30.661476</td>\n",
       "      <td>16.840320</td>\n",
       "      <td>...</td>\n",
       "      <td>41.077034</td>\n",
       "      <td>12.799846</td>\n",
       "      <td>-11.113871</td>\n",
       "      <td>5.447135</td>\n",
       "      <td>-18.942200</td>\n",
       "      <td>31.162622</td>\n",
       "      <td>62.247715</td>\n",
       "      <td>59.231285</td>\n",
       "      <td>8.824180</td>\n",
       "      <td>-11.954215</td>\n",
       "      <td>-34.684780</td>\n",
       "      <td>23.450802</td>\n",
       "      <td>-2.180560</td>\n",
       "      <td>-86.84554</td>\n",
       "      <td>99.28801</td>\n",
       "      <td>-26.345278</td>\n",
       "      <td>24.123669</td>\n",
       "      <td>-14.223212</td>\n",
       "      <td>-16.67198</td>\n",
       "      <td>-63.230910</td>\n",
       "      <td>-12.958088</td>\n",
       "      <td>-50.234467</td>\n",
       "      <td>-13.013007</td>\n",
       "      <td>-14.103242</td>\n",
       "      <td>42.61641</td>\n",
       "      <td>-21.69006</td>\n",
       "      <td>26.172844</td>\n",
       "      <td>11.369445</td>\n",
       "      <td>-33.764090</td>\n",
       "      <td>-67.079025</td>\n",
       "      <td>9.298884</td>\n",
       "      <td>19.294523</td>\n",
       "      <td>-8.583278</td>\n",
       "      <td>-10.85127</td>\n",
       "      <td>44.521545</td>\n",
       "      <td>34.764984</td>\n",
       "      <td>26.189013</td>\n",
       "      <td>-16.013206</td>\n",
       "      <td>55.043896</td>\n",
       "      <td>-30.604733</td>\n",
       "      <td>-28.991419</td>\n",
       "      <td>9.465713</td>\n",
       "      <td>10.260703</td>\n",
       "      <td>26.759874</td>\n",
       "      <td>0.781894</td>\n",
       "      <td>-47.584580</td>\n",
       "      <td>-28.393494</td>\n",
       "      <td>44.036330</td>\n",
       "      <td>-7.025197</td>\n",
       "      <td>25.417854</td>\n",
       "      <td>-7.101059</td>\n",
       "      <td>-13.295713</td>\n",
       "      <td>-5.699793</td>\n",
       "      <td>49.621677</td>\n",
       "      <td>24.702765</td>\n",
       "      <td>-11.901889</td>\n",
       "      <td>-49.367367</td>\n",
       "      <td>-19.016842</td>\n",
       "      <td>-60.045570</td>\n",
       "      <td>23.239857</td>\n",
       "      <td>-50.788387</td>\n",
       "      <td>-32.99513</td>\n",
       "      <td>18.065453</td>\n",
       "      <td>19.075115</td>\n",
       "      <td>1.297463</td>\n",
       "      <td>-44.421906</td>\n",
       "      <td>-18.859306</td>\n",
       "      <td>17.023155</td>\n",
       "      <td>25.250190</td>\n",
       "      <td>-55.614334</td>\n",
       "      <td>-28.540754</td>\n",
       "      <td>-10.573200</td>\n",
       "      <td>21.149540</td>\n",
       "      <td>1.372037</td>\n",
       "      <td>-5.163021</td>\n",
       "      <td>-26.236847</td>\n",
       "      <td>23.637201</td>\n",
       "      <td>55.501034</td>\n",
       "      <td>36.188070</td>\n",
       "      <td>-55.459106</td>\n",
       "      <td>10.345716</td>\n",
       "      <td>-0.111478</td>\n",
       "      <td>12.393797</td>\n",
       "      <td>18.552849</td>\n",
       "      <td>24.551563</td>\n",
       "      <td>-34.019210</td>\n",
       "      <td>40.915413</td>\n",
       "      <td>60.562595</td>\n",
       "      <td>-0.243935</td>\n",
       "      <td>-49.220146</td>\n",
       "      <td>-36.996803</td>\n",
       "      <td>26.591246</td>\n",
       "      <td>-64.819030</td>\n",
       "      <td>64.506140</td>\n",
       "      <td>-8.753573</td>\n",
       "      <td>4.985069</td>\n",
       "      <td>11.915058</td>\n",
       "      <td>-31.453096</td>\n",
       "      <td>-66.645706</td>\n",
       "      <td>0.948927</td>\n",
       "      <td>36.386353</td>\n",
       "      <td>-4.368111</td>\n",
       "      <td>31.776743</td>\n",
       "      <td>-39.42693</td>\n",
       "      <td>22.247760</td>\n",
       "      <td>-18.933329</td>\n",
       "      <td>-47.260550</td>\n",
       "      <td>27.978941</td>\n",
       "      <td>-42.787804</td>\n",
       "      <td>-14.799994</td>\n",
       "      <td>5.312534</td>\n",
       "      <td>23.324577</td>\n",
       "      <td>2.140388</td>\n",
       "      <td>18.434973</td>\n",
       "      <td>-4.537496</td>\n",
       "      <td>-6.831216</td>\n",
       "      <td>12.042288</td>\n",
       "      <td>28.417608</td>\n",
       "      <td>-20.447468</td>\n",
       "      <td>-64.81586</td>\n",
       "      <td>1.825137</td>\n",
       "      <td>65.804030</td>\n",
       "      <td>66.038360</td>\n",
       "      <td>22.061098</td>\n",
       "      <td>32.594086</td>\n",
       "      <td>1.450540</td>\n",
       "      <td>13.013606</td>\n",
       "      <td>30.462559</td>\n",
       "      <td>-25.140808</td>\n",
       "      <td>8.396724</td>\n",
       "      <td>19.161358</td>\n",
       "      <td>22.407690</td>\n",
       "      <td>-6.146007</td>\n",
       "      <td>-28.183273</td>\n",
       "      <td>-27.74174</td>\n",
       "      <td>2.022320</td>\n",
       "      <td>-7.757467</td>\n",
       "      <td>13.601197</td>\n",
       "      <td>17.816069</td>\n",
       "      <td>-4.900016</td>\n",
       "      <td>10.91540</td>\n",
       "      <td>34.488976</td>\n",
       "      <td>-31.581772</td>\n",
       "      <td>25.981552</td>\n",
       "      <td>-19.930105</td>\n",
       "      <td>54.52244</td>\n",
       "      <td>0.373021</td>\n",
       "      <td>-65.178280</td>\n",
       "      <td>-10.419116</td>\n",
       "      <td>33.777170</td>\n",
       "      <td>-29.364279</td>\n",
       "      <td>-6.325980</td>\n",
       "      <td>-3.957853</td>\n",
       "      <td>1.727094</td>\n",
       "      <td>-12.642872</td>\n",
       "      <td>43.225273</td>\n",
       "      <td>-16.881365</td>\n",
       "      <td>27.03476</td>\n",
       "      <td>27.907198</td>\n",
       "      <td>31.138374</td>\n",
       "      <td>-122.310356</td>\n",
       "      <td>60.787697</td>\n",
       "      <td>-49.262108</td>\n",
       "      <td>3.890955</td>\n",
       "      <td>-35.615820</td>\n",
       "      <td>24.183817</td>\n",
       "      <td>38.957893</td>\n",
       "      <td>75.20548</td>\n",
       "      <td>44.197160</td>\n",
       "      <td>8.190554</td>\n",
       "      <td>-16.537477</td>\n",
       "      <td>30.94674</td>\n",
       "      <td>20.293303</td>\n",
       "      <td>-25.050058</td>\n",
       "      <td>22.92504</td>\n",
       "      <td>8.011419</td>\n",
       "      <td>82.96680</td>\n",
       "      <td>-19.849014</td>\n",
       "      <td>9.840103</td>\n",
       "      <td>25.440804</td>\n",
       "      <td>10.999892</td>\n",
       "      <td>36.805660</td>\n",
       "      <td>15.351712</td>\n",
       "      <td>-29.362240</td>\n",
       "      <td>-58.28833</td>\n",
       "      <td>34.848774</td>\n",
       "      <td>-27.919823</td>\n",
       "      <td>-98.72599</td>\n",
       "      <td>75.90399</td>\n",
       "      <td>-65.617650</td>\n",
       "      <td>-12.502629</td>\n",
       "      <td>21.050995</td>\n",
       "      <td>8.527073</td>\n",
       "      <td>-48.275055</td>\n",
       "      <td>19.199772</td>\n",
       "      <td>-45.139156</td>\n",
       "      <td>36.625458</td>\n",
       "      <td>2.041568</td>\n",
       "      <td>-62.008595</td>\n",
       "      <td>-19.786633</td>\n",
       "      <td>-39.888690</td>\n",
       "      <td>69.219360</td>\n",
       "      <td>48.080235</td>\n",
       "      <td>-4.232124</td>\n",
       "      <td>66.18564</td>\n",
       "      <td>16.264645</td>\n",
       "      <td>-36.72925</td>\n",
       "      <td>-11.218119</td>\n",
       "      <td>11.580771</td>\n",
       "      <td>61.334090</td>\n",
       "      <td>-44.44460</td>\n",
       "      <td>-12.594831</td>\n",
       "      <td>-35.713140</td>\n",
       "      <td>66.84117</td>\n",
       "      <td>-9.298966</td>\n",
       "      <td>-34.992390</td>\n",
       "      <td>52.199830</td>\n",
       "      <td>-7.717034</td>\n",
       "      <td>59.813793</td>\n",
       "      <td>-50.728607</td>\n",
       "      <td>18.414217</td>\n",
       "      <td>-14.535299</td>\n",
       "      <td>-30.329940</td>\n",
       "      <td>-4.355602</td>\n",
       "      <td>31.999712</td>\n",
       "      <td>-35.288208</td>\n",
       "      <td>-4.302416</td>\n",
       "      <td>-0.109789</td>\n",
       "      <td>11.729748</td>\n",
       "      <td>6.401609</td>\n",
       "      <td>-10.317232</td>\n",
       "      <td>33.045418</td>\n",
       "      <td>-5.182931</td>\n",
       "      <td>-3.601586</td>\n",
       "      <td>0.951922</td>\n",
       "      <td>-36.313385</td>\n",
       "      <td>19.594986</td>\n",
       "      <td>-24.099216</td>\n",
       "      <td>40.261723</td>\n",
       "      <td>-30.399551</td>\n",
       "      <td>-51.330740</td>\n",
       "      <td>25.773672</td>\n",
       "      <td>27.008630</td>\n",
       "      <td>-43.414970</td>\n",
       "      <td>10.715737</td>\n",
       "      <td>-20.651213</td>\n",
       "      <td>42.569336</td>\n",
       "      <td>-62.557750</td>\n",
       "      <td>46.186752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3         4          5          6          7          8          9         10       11         12         13        14         15         16         17         18         19         20         21         22         23         24         25         26         27         28         29         30        31         32         33         34         35         36         37         38        39         40         41         42         43          44         45         46         47         48         49         50         51         52        53         54         55         56         57        58         59        60         61         62         63         64         65         66         67         68         69        70         71         72         73         74         75         76         77         78          79        80         81         82         83        84         85         86         87         88         89        90         91         92         93         94         95         96         97         98          99        100       101         102       103        104       105        106        107        108        109        110       111        112        113        114        115        116        117        118        119       120        121       122        123        124       125        126        127        128        129        130        131       132       133        134        135       136       137        138       139        140        141        142        143        144        145        146        147        148        149        150        151       152        153        154        155        156        157        158        159       160        161        162        163        164        165        166       167       168        169         170        171        172        173        174       175        176        177        178        179        180        181       182        183        184        185        186        187        188         189        190       191        192       193        194        195        196       197       198        199        200        201        202       203        204        205       206       207        208        209        210        211        212        213        214        215        216       217        218        219       220        221        222        223       224        225        226        227        228       229        230       231        232        233        234        235        236        237        238       239        240        241        242        243        244       245        246       247        248        249  ...        263        264        265       266        267        268        269        270       271        272        273        274        275       276       277        278        279        280       281        282        283        284        285        286       287       288        289        290        291        292       293        294        295       296        297        298        299        300        301        302        303        304        305        306        307        308        309        310        311        312       313        314        315        316        317        318        319        320        321        322        323       324        325        326       327        328        329        330        331        332        333        334        335       336       337        338        339        340        341        342        343        344        345        346        347        348        349        350        351        352         353        354        355        356       357       358        359        360        361        362        363        364        365       366        367        368        369        370        371        372       373        374        375        376        377        378        379        380        381       382       383        384        385        386        387        388        389        390        391        392        393        394       395        396       397       398        399        400        401       402       403        404        405        406        407       408        409        410        411        412        413        414       415       416        417         418        419       420        421        422         423        424        425       426        427        428        429        430        431        432        433       434        435        436       437       438       439        440        441        442        443        444        445        446       447        448        449       450        451        452        453        454        455        456        457        458        459       460        461        462        463        464        465       466       467        468       469        470        471        472       473        474        475       476       477        478        479        480        481        482        483        484        485        486        487        488        489        490        491        492        493        494        495       496        497        498        499        500        501        502        503        504        505        506        507        508        509        510        511  class\n",
       "0  34.076153 -29.068226  57.540318 -51.954987  3.320737  -5.768854  79.962100  39.567856 -17.904541 -13.444562   7.104392 -55.4440 -24.303596 -44.187996  2.927974  53.233093   50.92992 -43.447510 -89.645250  22.505268 -60.829258 -20.836334  -0.965031  31.152092 -11.122220 -25.034784   4.409427  26.100441 -25.010174 -74.720795  53.209137 -81.84636  50.065517 -45.970085  30.723158  27.164768 -28.826418  -6.222152   6.740042  40.17959  44.086067  19.706493  20.024275 -64.465230   59.972400  -8.002469  13.231766  92.479520  59.146214  36.834324 -39.294914  38.488094 -15.634628  5.950771  64.343360 -57.710840 -31.729551  17.203966  6.631249  42.381054  2.142177   8.529372 -37.259970   6.108812 -27.347390  26.856197 -21.978996 -52.820110  53.621487   2.217431  81.62559  19.125967 -12.114867  114.19409  22.398640 -26.302088   9.557514 -63.690918  -6.997769  -72.960580 -62.32190  38.198235 -22.560627  58.402110  6.116537 -55.134212  10.474427 -24.749079  24.216830  76.149510 -57.48442  44.822865  10.780519 -26.609621  -5.474360 -58.219970  -2.624526 -48.936640  -8.839552  115.739075  11.119407  43.98946   64.425964 -41.28047  26.402365  47.93493 -28.618982 -32.531040  25.251757   7.147524 -20.038187 -6.329055 -82.623290 -19.123924  19.217548  24.678738 -46.619442 -16.176466  53.027214 -56.030000 -5.062048 -20.987423  2.937074 -38.340145  10.470112  59.40322 -51.526802  22.722168   9.580302  71.415770 -54.483715 -29.626957 -5.157812 -62.49628  38.692795  -0.546269 -5.400824 -69.03352  34.485413  34.81170   4.540720  14.029618   1.117358  67.148766  54.817284 -21.200008 -47.789494  38.494070 -79.732940  37.797665 -44.171960  42.925446  31.51484 -22.553644  33.205990 -22.327332  24.549444  13.433725 -25.152117  22.379246   8.82339  -5.749085 -32.831110  14.075165 -13.587171 -24.091106   3.647905 -49.28312 -73.42274  34.545450  121.210620   6.579218 -14.331875  25.185430  -0.427416  25.58867  32.091415  60.210037  114.56279 -18.183249  109.43097   4.606071 -816.7640  34.536625  -1.968195 -28.142189 -35.985890 -26.224033 -14.747208  -44.331577  40.095245  72.64052  22.202890  44.85098  45.743100 -17.891928 -69.220380  9.521957 -46.55743  38.384500 -22.983368  36.421482  -1.850905  53.13959  27.446419  54.202442 -41.36680 -74.47336 -37.391410  26.398663   5.670536  -7.041967 -72.567800  59.946780 -25.505404 -37.007404 -20.950542 -89.56310  46.319607  39.164486 -8.790081 -44.477100  -7.110732   5.267138  38.50156  20.837385 -16.691378  15.983549 -34.441160  5.828382  -0.527457 -89.16502 -11.465632 -41.326492  48.355556  10.083818  -6.473544  56.162754  -7.291161 -43.13583  -7.445608  15.005452 -21.107140 -22.295748  54.071922  0.270911 -13.642708 -8.504085  73.680405  11.293367  ...  44.443905  -6.000172 -13.519516  0.777792 -45.398483  21.852337  46.511040  47.308490  7.052961 -59.692270 -45.581566  -5.164234 -53.656662 -67.76812  56.04384  -8.995872  27.532810  -5.376409   4.35754 -17.950928 -34.384330 -33.412437   2.268966 -19.778896  32.21143 -96.01068  15.489947 -20.747654 -56.013233 -45.682384  2.426109  28.742384 -43.443092  -9.90637  41.973927   1.298070  60.387585   5.639635  33.382930  54.166416 -27.001698 -33.079388  32.099163 -13.426831 -25.494100   2.569302 -19.967075  27.034811 -27.587006   3.497190 -2.361070  18.953096 -35.018530  15.905002  52.972460  10.520672  -6.272368  21.481680 -49.247795   9.748928 -20.084112  -8.60420  44.910910 -18.825981  4.937567 -77.733635  -8.999736  22.257694  30.929598 -62.188572 -19.494125  24.403084  19.481556  8.760558  6.620137  -8.348998  50.268090  50.917107  17.995626 -48.919400 -14.037656  13.549075  -0.513817 -14.460697  35.509663 -21.761173  54.061058  27.399305 -18.618568  -3.328788 -104.211910  -0.355629 -64.095924  30.758144  0.901769  0.102869  -1.056369 -42.882874 -55.050808  15.005190  17.073736 -12.501462 -19.862846 -24.29204  35.211872 -11.243217 -12.755215  66.646645 -32.807500  -0.189058 -1.808646  27.252577 -35.074340  28.530462 -29.252598 -21.704420  11.736714  54.281868 -18.355640 -43.98375  5.979135  59.810116  24.799034  12.219104  56.225810  17.381193  -8.651945   8.412357 -40.141530  20.355501  22.359621  33.184193 -9.555579 -28.605461 -44.75495 -2.173939 -31.205430   4.260418  -1.839805  1.392822   7.33269  53.761410 -16.712246 -23.574620 -22.573244  24.96579  14.559516 -14.097199 -24.911524  -1.114475 -48.574276  11.136024  1.976270 -9.241048  -6.975331  103.856870  15.996011  46.73991  40.200363  24.503580 -127.010635  68.728720  -8.694561 -2.824552 -34.471985  21.793420  35.863308  105.52343  49.373283 -14.536914  12.671607  29.56023  41.027367 -22.932981  47.56592 -6.551812  76.19963  -7.192747 -14.825335  11.637272  11.242970  48.271664  16.139172 -45.867954 -52.42260   1.913857 -84.181510 -91.65905  102.68495 -63.827293  -0.242578  38.447304  24.781075 -59.375990  -7.122683  10.546757   7.254651  7.789337 -23.236930  28.699804 -19.570255  64.782646  56.858818  3.428119  42.64988 -25.013424 -33.71716 -14.538428  21.464668  74.353615 -42.33282  -9.540282 -20.122068  42.71078 -0.752739 -13.769701   1.050589  34.419098  29.317112 -38.019440  16.762892   9.687283 -55.027912 -35.494670  58.193813  -1.021184 -32.058918  44.992233  -0.932869  30.857075 -11.946971   6.736805 -10.248713 -4.045509 -20.385088 -15.440907  29.357740 -52.567047  35.755077 -52.902596 -19.132236  24.880768  31.330477 -17.345932  18.819689  -8.718418   4.374234 -51.202106   5.171546      0\n",
       "1  72.869040   9.177978   9.704573 -71.219390 -5.944699 -20.537598  48.504128  59.785946 -27.258816 -32.251190  14.800838 -53.5956 -42.787678 -24.545757 -0.176815  19.415184  109.40230  22.388596 -46.119553  12.120032 -47.389282 -47.334510 -47.276146  20.823063 -23.905811  14.022844 -23.051254  53.248184 -53.657482 -87.474130  45.754990 -88.19240  41.973026 -32.760735 -23.768059  48.279350 -18.068876 -62.380000 -25.564194  61.33734  40.179916  20.387558  30.500923 -60.822758  102.107605  14.142278  -3.871582  42.111538   6.905591   4.958260  16.016796   7.780636  -0.848523  1.473870  57.682693 -49.853676 -19.702677  31.856785 -6.906637  13.091820  9.293388 -46.937810 -15.624641  34.178690  -8.953811   8.842891 -27.109920 -33.114067  53.697777  23.613077  54.73936 -19.626383 -10.744253  132.13037  47.727695  -5.566465  19.573042 -29.790215  14.902081 -118.077576 -66.97544  -5.061472 -26.892975  20.348545 -8.329653 -39.616898  46.656700  19.301174  25.580462  40.807236 -46.38029  58.307915  22.420975 -42.110580 -43.163948 -36.296097 -14.917341  -7.593774  18.175910  102.734770  17.544197  49.27179  108.209670 -51.23678 -16.084090  71.24989 -16.147522 -25.371195  -0.574685  38.962357  23.742937 -7.922504 -49.855923  -5.934124   0.081892  -3.771686 -74.046230 -50.868767  46.930702  23.048883  4.499016 -34.497400 -6.409646  19.599749  -2.481324  43.26506 -43.030330 -45.001186  35.294243   5.455137 -72.468380 -42.423283 -9.063019 -10.23430  37.078854  21.051090 -7.432247 -83.89124  25.783722 -30.41206  35.521923  17.499136  62.885590  23.285479  45.144318 -17.518013 -22.365147  44.583626 -30.976406 -25.191942 -30.897219  49.849342  48.11522  -5.820929  12.154154 -14.323309  50.116608  17.387451 -28.413244   2.304046  38.76875  15.832017 -11.301152  14.961336  -5.422130 -51.911636  55.641655 -36.55812 -78.60230  27.394987  121.336235 -37.578762 -14.832090  23.933712  18.173290  32.73622  44.363930  23.287400   84.99884 -13.705014  101.15838  18.020290 -841.8158  40.813717  16.592876 -18.084003 -32.998116 -41.458576   1.810637 -100.587960  -7.341225  79.67986   1.496814  20.61357  44.502132   0.104825   4.125615 -1.947869 -44.67108  29.651737 -30.925812  59.398655  35.231987  58.75975  46.080510   3.883729 -66.10764 -43.79035 -44.196465  19.362597  11.944158 -22.527617 -36.538193  25.043571 -30.555069 -79.037575 -36.023853 -58.09647  -6.596299  90.721985 -4.542720 -25.315422  28.636532 -23.953516  43.03536   9.448089  17.827640  21.234476  -6.425253  4.652818  14.221005 -73.50012   3.506240 -34.049060  68.434860 -15.345180 -47.916664  58.037685  12.731979 -38.65578  19.502857  16.036879  29.603859 -19.430283  -5.402806 -1.403189 -32.265370  7.675438  30.661476  16.840320  ...  41.077034  12.799846 -11.113871  5.447135 -18.942200  31.162622  62.247715  59.231285  8.824180 -11.954215 -34.684780  23.450802  -2.180560 -86.84554  99.28801 -26.345278  24.123669 -14.223212 -16.67198 -63.230910 -12.958088 -50.234467 -13.013007 -14.103242  42.61641 -21.69006  26.172844  11.369445 -33.764090 -67.079025  9.298884  19.294523  -8.583278 -10.85127  44.521545  34.764984  26.189013 -16.013206  55.043896 -30.604733 -28.991419   9.465713  10.260703  26.759874   0.781894 -47.584580 -28.393494  44.036330  -7.025197  25.417854 -7.101059 -13.295713  -5.699793  49.621677  24.702765 -11.901889 -49.367367 -19.016842 -60.045570  23.239857 -50.788387 -32.99513  18.065453  19.075115  1.297463 -44.421906 -18.859306  17.023155  25.250190 -55.614334 -28.540754 -10.573200  21.149540  1.372037 -5.163021 -26.236847  23.637201  55.501034  36.188070 -55.459106  10.345716  -0.111478  12.393797  18.552849  24.551563 -34.019210  40.915413  60.562595  -0.243935 -49.220146  -36.996803  26.591246 -64.819030  64.506140 -8.753573  4.985069  11.915058 -31.453096 -66.645706   0.948927  36.386353  -4.368111  31.776743 -39.42693  22.247760 -18.933329 -47.260550  27.978941 -42.787804 -14.799994  5.312534  23.324577   2.140388  18.434973  -4.537496  -6.831216  12.042288  28.417608 -20.447468 -64.81586  1.825137  65.804030  66.038360  22.061098  32.594086   1.450540  13.013606  30.462559 -25.140808   8.396724  19.161358  22.407690 -6.146007 -28.183273 -27.74174  2.022320  -7.757467  13.601197  17.816069 -4.900016  10.91540  34.488976 -31.581772  25.981552 -19.930105  54.52244   0.373021 -65.178280 -10.419116  33.777170 -29.364279  -6.325980 -3.957853  1.727094 -12.642872   43.225273 -16.881365  27.03476  27.907198  31.138374 -122.310356  60.787697 -49.262108  3.890955 -35.615820  24.183817  38.957893   75.20548  44.197160   8.190554 -16.537477  30.94674  20.293303 -25.050058  22.92504  8.011419  82.96680 -19.849014   9.840103  25.440804  10.999892  36.805660  15.351712 -29.362240 -58.28833  34.848774 -27.919823 -98.72599   75.90399 -65.617650 -12.502629  21.050995   8.527073 -48.275055  19.199772 -45.139156  36.625458  2.041568 -62.008595 -19.786633 -39.888690  69.219360  48.080235 -4.232124  66.18564  16.264645 -36.72925 -11.218119  11.580771  61.334090 -44.44460 -12.594831 -35.713140  66.84117 -9.298966 -34.992390  52.199830  -7.717034  59.813793 -50.728607  18.414217 -14.535299 -30.329940  -4.355602  31.999712 -35.288208  -4.302416  -0.109789  11.729748   6.401609 -10.317232  33.045418  -5.182931 -3.601586   0.951922 -36.313385  19.594986 -24.099216  40.261723 -30.399551 -51.330740  25.773672  27.008630 -43.414970  10.715737 -20.651213  42.569336 -62.557750  46.186752      0\n",
       "\n",
       "[2 rows x 513 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertVectors_fulldf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80e0ad30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20820\n",
       "1    17450\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertVectors_fulldf['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertVectors_fulldf.to_csv('Updated//bertVectors_fulldf_ISOT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931c393",
   "metadata": {},
   "source": [
    "### Loading dataframe BertVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709aef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertVectors_fulldf = pd.read_csv(\"Updated//bertVectors_fulldf_ISOT.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b5229",
   "metadata": {},
   "source": [
    "### Splitting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fa3f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=bertVectors_fulldf.drop('class',axis=1)\n",
    "y=bertVectors_fulldf['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8bdf3",
   "metadata": {},
   "source": [
    "#### Using StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8434f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e798288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in kfold.split(X, y):\n",
    "    # select rows\n",
    "    X_train, X_test = X.iloc[i], X.iloc[j]\n",
    "    y_train, y_test = y.iloc[i], y.iloc[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "215133f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7654, 512)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39579e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c7bb9a4",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b832779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    " \n",
    "\n",
    "\n",
    "def metrics(X_test,y_test,clf):\n",
    "    predictions=clf.predict(X_test)\n",
    "    #predictions=(clf.predict_proba(X_test)[:,1] >= 0.3).astype(bool)\n",
    "    print(\"confusion_matrix :\")\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print(\"Accuracy Score :\")\n",
    "    print(accuracy_score(y_test, predictions))\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"F1 score :\")\n",
    "    print(f1_score(y_test, predictions))\n",
    "    print(\"ROC AUC Score\")\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "    print(roc_auc_score(y_test, y_pred_proba[:,1]) )\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    \n",
    "def model_comparison_table(X_test,y_test,classifier_list):\n",
    "    dict_clf={}\n",
    "    for clf_name,clf in classifier_list:\n",
    "        predictions=clf.predict(X_test)\n",
    "        y_pred_proba = clf.predict_proba(X_test)\n",
    "        accuracy=accuracy_score(y_test, predictions)\n",
    "        precision=precision_score(y_test,predictions,average='macro').round(2)\n",
    "        recall=recall_score(y_test,predictions)\n",
    "        f1score=f1_score(y_test,predictions).round(2)\n",
    "        ROC_AUC=roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "        dict_clf[clf_name]=[accuracy,precision,recall,f1score,ROC_AUC]\n",
    "    df_models_scores = pd.DataFrame(dict_clf, index=['Accuracy', 'Precision', 'Recall', 'F1 Score','roc_auc_score'])\n",
    "    return df_models_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fc89713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helper Functions\n",
    "\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    " \n",
    "\n",
    "\n",
    "def metrics(X_test,y_test,clf):\n",
    "    predictions=clf.predict(X_test)\n",
    "    #predictions=(clf.predict_proba(X_test)[:,1] >= 0.3).astype(bool)\n",
    "    print(\"confusion_matrix :\")\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print(\"Accuracy Score :\")\n",
    "    print(accuracy_score(y_test, predictions))\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"F1 score :\")\n",
    "    print(f1_score(y_test, predictions))\n",
    "    print(\"ROC AUC Score\")\n",
    "    y_pred_proba = clf.predict_proba(X_test)\n",
    "    print(roc_auc_score(y_test, y_pred_proba[:,1]) )\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    \n",
    "def model_comparison_table(X_test,y_test,classifier_list):\n",
    "    dict_clf={}\n",
    "    for clf_name,clf in classifier_list:\n",
    "        predictions=clf.predict(X_test)\n",
    "        y_pred_proba = clf.predict_proba(X_test)\n",
    "        accuracy=accuracy_score(y_test, predictions)\n",
    "        precision=precision_score(y_test,predictions,average='macro').round(2)\n",
    "        recall=recall_score(y_test,predictions)\n",
    "        f1score=f1_score(y_test,predictions).round(2)\n",
    "        ROC_AUC=roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "        dict_clf[clf_name]=[accuracy,precision,recall,f1score,ROC_AUC]\n",
    "    df_models_scores = pd.DataFrame(dict_clf, index=['Accuracy', 'Precision', 'Recall', 'F1 Score','roc_auc_score'])\n",
    "    return df_models_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83306eb",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3a9e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a basic Random Forest model on top of the vectors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a223a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.9380715965508231\n",
      "Confusion matrix : \n",
      " [[3992  172]\n",
      " [ 302 3188]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      4164\n",
      "           1       0.95      0.91      0.93      3490\n",
      "\n",
      "    accuracy                           0.94      7654\n",
      "   macro avg       0.94      0.94      0.94      7654\n",
      "weighted avg       0.94      0.94      0.94      7654\n",
      "\n",
      "Precision : 0.9488095238095238\n",
      "Recall : 0.9134670487106017\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precision : {}\".format(precision))\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall : {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457e97d",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae0716c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf=SVC(random_state=0,probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd368ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.9760909328455709\n",
      "Confusion matrix : \n",
      " [[4089   75]\n",
      " [ 108 3382]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      4164\n",
      "           1       0.98      0.97      0.97      3490\n",
      "\n",
      "    accuracy                           0.98      7654\n",
      "   macro avg       0.98      0.98      0.98      7654\n",
      "weighted avg       0.98      0.98      0.98      7654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "pred = svm_clf.predict(X_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12664f9b",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33653bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create KNN Object.\n",
    "knn_clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ab8f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.9517899137705774\n",
      "Confusion matrix : \n",
      " [[4064  100]\n",
      " [ 269 3221]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      4164\n",
      "           1       0.97      0.92      0.95      3490\n",
      "\n",
      "    accuracy                           0.95      7654\n",
      "   macro avg       0.95      0.95      0.95      7654\n",
      "weighted avg       0.95      0.95      0.95      7654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_clf.fit(X_train,y_train)\n",
    "\n",
    "pred = knn_clf.predict(X_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f878c31",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ac17299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.9668147373922132\n",
      "Confusion matrix : \n",
      " [[4065   99]\n",
      " [ 155 3335]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97      4164\n",
      "           1       0.97      0.96      0.96      3490\n",
      "\n",
      "    accuracy                           0.97      7654\n",
      "   macro avg       0.97      0.97      0.97      7654\n",
      "weighted avg       0.97      0.97      0.97      7654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create KNN Object.\n",
    "xg_clf=XGBClassifier(random_state=0)\n",
    "\n",
    "xg_clf.fit(X_train,y_train)\n",
    "\n",
    "pred = xg_clf.predict(X_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3927128d",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3cb40fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.9178207473216619\n",
      "Confusion matrix : \n",
      " [[3895  269]\n",
      " [ 360 3130]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      4164\n",
      "           1       0.92      0.90      0.91      3490\n",
      "\n",
      "    accuracy                           0.92      7654\n",
      "   macro avg       0.92      0.92      0.92      7654\n",
      "weighted avg       0.92      0.92      0.92      7654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf=GaussianNB()\n",
    "#p = Pipeline([('Normalizing',MinMaxScaler()),('MultinomialNB',MultinomialNB())])\n",
    "nb_clf.fit(X_train,y_train) \n",
    "\n",
    "pred = nb_clf.predict(X_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d989f",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ad9f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f69ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.9822315129344134\n",
      "Confusion matrix : \n",
      " [[4097   67]\n",
      " [  69 3421]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      4164\n",
      "           1       0.98      0.98      0.98      3490\n",
      "\n",
      "    accuracy                           0.98      7654\n",
      "   macro avg       0.98      0.98      0.98      7654\n",
      "weighted avg       0.98      0.98      0.98      7654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logticreg_clf=LogisticRegression()\n",
    "\n",
    "logticreg_clf.fit(X_train,y_train)\n",
    "\n",
    "pred = logticreg_clf.predict(X_test)\n",
    "print(\"Accuracy score : {}\".format(accuracy_score(y_test, pred)))\n",
    "print(\"Confusion matrix : \\n {}\".format(confusion_matrix(y_test, pred)))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065cc84",
   "metadata": {},
   "source": [
    "### Consolidating all classifiers accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "436316f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [('Logistic Regression',logticreg_clf),('Naive Bayes',nb_clf),('Xgboost',xg_clf),\n",
    "              ('KNN',knn_clf),('Random Forest',rf_model),(\"SVM\",svm_clf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7df3657c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.982232</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.980229</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.998288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.976091</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.969054</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.997531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgboost</th>\n",
       "      <td>0.966815</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.955587</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.995014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.951790</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.922923</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.984113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.938072</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.913467</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.984804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.917821</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.896848</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.968778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall  F1 Score  roc_auc_score\n",
       "Logistic Regression  0.982232       0.98  0.980229      0.98       0.998288\n",
       "SVM                  0.976091       0.98  0.969054      0.97       0.997531\n",
       "Xgboost              0.966815       0.97  0.955587      0.96       0.995014\n",
       "KNN                  0.951790       0.95  0.922923      0.95       0.984113\n",
       "Random Forest        0.938072       0.94  0.913467      0.93       0.984804\n",
       "Naive Bayes          0.917821       0.92  0.896848      0.91       0.968778"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_scores=model_comparison_table(X_test,y_test,classifiers)\n",
    "df_model_scores.head(20).T.sort_values(by='Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4dafcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
