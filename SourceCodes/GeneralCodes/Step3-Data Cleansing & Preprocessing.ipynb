{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d3cb6c",
   "metadata": {},
   "source": [
    "# 1. Dataset Cleansing \n",
    "\n",
    "***\n",
    "\n",
    "This notebooks includes the general scripts regarding the data cleansing which we applied over the datasets for the model development of fake news automatic detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69307c61",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Script to extract or convert  attributes from JSON  News Content format to  dataframe\n",
    "\n",
    "This script is using to extract multiple attributes of the news content from the JSON file  and filtered the domain names from the url attribute  and created a new dataframe.\n",
    "\n",
    "This below script applied over the FakeNewsNet Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65d5fc1c",
   "metadata": {},
   "source": [
    "def json_to_newscontent(datapath):\n",
    "    dictlist = []\n",
    "    cols = ['url','text','title','authors','num_images','domain']\n",
    "    folders = glob.glob(datapath+'/*')\n",
    "    for index, subdir in enumerate(folders):\n",
    "        path_file = glob.glob(subdir+'/*')\n",
    "        #check whether file path is valid or not\n",
    "        if len(path_file) == 1:\n",
    "            file = open(path_file[0]).read()\n",
    "            jsondata = json.loads(file)\n",
    "            thedict = {'url':jsondata['url'],'title':jsondata['title'],'text':jsondata['text'],\n",
    "                   'num_images':len(jsondata['images']),'authors':str(jsondata['authors'])}\n",
    "            extrt_url = tldextract.extract(jsondata['url'])\n",
    "            thedict['domain'] = extrt_url.domain\n",
    "            dictlist.append(thedict)\n",
    "    df=pd.DataFrame(dictlist,columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f191dee",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### 1.2 Removing rows having missing values\n",
    "\n",
    "We checked the data to find out any missing values available in the news text article and removed those having missing values \n",
    "\n",
    "**Sample Script**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "122a73e2",
   "metadata": {},
   "source": [
    "df_full = df_full[df_full['title'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931b7b3",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### 1.3 Removing duplicate rows\n",
    "\n",
    "We removed the duplicate rows of  news title and texts from the dataset\n",
    "\n",
    "**Sample Script**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0a7d10d",
   "metadata": {},
   "source": [
    "isot_full_df = isot_full_df.drop_duplicates(subset = ['text'])\n",
    "isot_full_df = isot_full_df.drop_duplicates(subset = ['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe0212",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52faf479",
   "metadata": {},
   "source": [
    "# 2. Text Data Preprocessing \n",
    "\n",
    "\n",
    "### Preprocessing with Cleaning Process\n",
    "\n",
    "\n",
    "Preprocessing text is always required during a text based classification model creation . Since text data always appears to be in unstructured raw data and it is not possible to feed the same format directly to model creation. We should cleanse the data and make it to a proper convention.\n",
    "\n",
    "We have experimented   the following pre-processing methods in our classification work:\n",
    "\n",
    "- Removal any HTML content\n",
    "- Remove URLs and numbers\n",
    "- Removal of all kinds of date formats\n",
    "- Removal of Punctuation \n",
    "- Conversion of lower case\n",
    "- Replacing 2 or more consecutive whitespaces with a single one\n",
    "- Removal of Stopwords\n",
    "- Lemmatization\n",
    "- Stemming\n",
    "- POS tagging \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff8511",
   "metadata": {},
   "source": [
    "#### Helper Functions to preprocess the text  with all the operations as mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b62395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tag POS tagging for each news content\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def text_preprocess(text):\n",
    "\n",
    "    # Remove HTML tags\n",
    "    bsoup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean_text = bsoup.get_text()\n",
    "    \n",
    "    \n",
    "    # Remove any URL\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    url.sub(r'', clean_text)\n",
    "    \n",
    "    # Remove any numbers\n",
    "    clean_text=re.sub(r'\\d+','',clean_text)\n",
    "    \n",
    "    # Remove all kinds of date formats\n",
    "    clean_text=re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', clean_text)\n",
    "    clean_text=re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', clean_text)\n",
    "    clean_text=re.sub(r'\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+', '', clean_text)\n",
    "    \n",
    "    \n",
    "    # Removal of punctuation and lower case conversion\n",
    "    clean_text = re.sub('\\[[^]]*\\]', ' ', clean_text)\n",
    "    clean_text = re.sub('[^a-zA-Z]',' ',clean_text)  # replaces non-alphabets with spaces\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # Removal of 2 consecutive double space\n",
    "    clean_text=re.sub(r' {2,}',' ',clean_text)\n",
    "    \n",
    "    # Removal of stop words\n",
    "    word_tokens = word_tokenize(clean_text) \n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    newcleantext = [w.strip() for w in word_tokens if w not in total_stop_words_list and len(w) > 2] \n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    newcleantext= [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in newcleantext]\n",
    "    \n",
    "    ### Again applying the removal of stop words\n",
    "     \n",
    "    newcleantext = [w.strip() for w in newcleantext if w not in total_stop_words_list and len(w) > 2] \n",
    "    \n",
    "    ## Removing duplicates\n",
    "    newcleantext = sorted(set(newcleantext), key=lambda x:newcleantext.index(x))\n",
    "    \n",
    "    return newcleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46119740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e0997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734123f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
